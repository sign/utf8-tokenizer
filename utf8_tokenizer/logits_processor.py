"""
UTF-8 Validation Logits Processor for HuggingFace transformers.

This module provides a LogitsProcessor that ensures generated sequences
form valid UTF-8 byte sequences by masking out invalid continuations.
"""

import torch
from transformers import LogitsProcessor


class UTF8ValidationLogitsProcessor(LogitsProcessor):
    """
    LogitsProcessor that enforces valid UTF-8 byte sequences during generation.

    This processor examines the previously generated bytes and masks out invalid
    next bytes according to UTF-8 encoding rules. This prevents the model from
    generating malformed UTF-8 sequences.

    UTF-8 encoding rules (from Unicode Technical Committee, 2025, ยง3.9.3):
    - 1-byte: 00-7F (ASCII)
    - 2-byte: C2-DF followed by 80-BF
    - 3-byte:
      - E0 followed by A0-BF, then 80-BF
      - E1-EC followed by 80-BF, then 80-BF
      - ED followed by 80-9F, then 80-BF
      - EE-EF followed by 80-BF, then 80-BF
    - 4-byte:
      - F0 followed by 90-BF, then 80-BF, then 80-BF
      - F1-F3 followed by 80-BF, then 80-BF, then 80-BF
      - F4 followed by 80-8F, then 80-BF, then 80-BF

    Usage:
        from transformers import AutoModelForCausalLM
        from utf8_tokenizer.logits_processor import UTF8ValidationLogitsProcessor

        model = AutoModelForCausalLM.from_pretrained("your-model")
        processor = UTF8ValidationLogitsProcessor()

        outputs = model.generate(
            input_ids,
            logits_processor=[processor],
            max_new_tokens=100
        )
    """

    def __init__(self):
        """Initialize the UTF-8 validation logits processor."""
        super().__init__()

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        """
        Process logits to enforce valid UTF-8 sequences.

        Args:
            input_ids: Previously generated token IDs of shape (batch_size, seq_len)
            scores: Logits for next token of shape (batch_size, vocab_size)

        Returns:
            Modified scores with invalid UTF-8 continuations masked to -inf
        """
        batch_size = input_ids.shape[0]
        vocab_size = scores.shape[1]

        # Create a mask initialized to False (all tokens allowed by default)
        mask = torch.zeros((batch_size, vocab_size), dtype=torch.bool, device=scores.device)

        for batch_idx in range(batch_size):
            # Get the UTF-8 state by examining the last few bytes
            seq = input_ids[batch_idx]
            allowed_bytes = self._get_allowed_next_bytes(seq)

            # Mask out all bytes that are not allowed
            for byte_val in range(256):
                if byte_val not in allowed_bytes:
                    mask[batch_idx, byte_val] = True

        # Set masked positions to -inf
        scores = scores.masked_fill(mask, float("-inf"))

        return scores

    def _get_allowed_next_bytes(self, sequence: torch.Tensor) -> set[int]:
        """
        Determine which bytes are valid as the next byte based on the sequence.

        Args:
            sequence: Tensor of previously generated byte IDs

        Returns:
            Set of allowed byte values (0-255)
        """
        if len(sequence) == 0:
            # At the start, we can generate any valid UTF-8 start byte
            return self._valid_start_bytes()

        # Convert to list for easier manipulation
        seq_list = sequence.tolist()

        # Find the start of the current incomplete UTF-8 sequence
        # by looking backwards from the end
        utf8_state = self._get_utf8_state(seq_list)

        if utf8_state["complete"]:
            # Current sequence is complete, can start a new character
            return self._valid_start_bytes()
        else:
            # In the middle of a multi-byte sequence
            return self._valid_continuation_bytes(utf8_state)

    def _valid_start_bytes(self) -> set[int]:
        """Return all valid UTF-8 starting bytes."""
        valid = set()

        # 1-byte sequences: 00-7F (ASCII)
        valid.update(range(0x00, 0x80))

        # 2-byte sequences: C2-DF
        valid.update(range(0xC2, 0xE0))

        # 3-byte sequences: E0-EF
        valid.update(range(0xE0, 0xF0))

        # 4-byte sequences: F0-F4
        valid.update(range(0xF0, 0xF5))

        return valid

    def _get_utf8_state(self, seq_list: list[int]) -> dict:  # noqa: C901
        """
        Analyze the sequence to determine the current UTF-8 state.

        Args:
            seq_list: List of byte values

        Returns:
            Dictionary with:
            - complete: bool indicating if current character is complete
            - first_byte: the first byte of the current sequence (if incomplete)
            - position: position within the multi-byte sequence (1, 2, or 3 for continuation)

        Note: Complexity is inherent to UTF-8 specification with multiple byte sequence types.
        """
        if not seq_list:
            return {"complete": True}

        # Look backwards to find the start of the current UTF-8 character
        for i in range(len(seq_list) - 1, -1, -1):
            byte_val = seq_list[i]

            # Check if this is a UTF-8 start byte
            if byte_val < 0x80:
                # ASCII - complete
                if i == len(seq_list) - 1:
                    return {"complete": True}
                else:
                    # We found an ASCII byte before the end, so the last bytes must be invalid
                    # This shouldn't happen with proper validation, but treat as complete
                    return {"complete": True}

            elif 0xC2 <= byte_val < 0xE0:
                # 2-byte sequence start
                bytes_after = len(seq_list) - i - 1
                if bytes_after >= 1:
                    return {"complete": True}
                else:
                    return {"complete": False, "first_byte": byte_val, "position": 1}

            elif 0xE0 <= byte_val < 0xF0:
                # 3-byte sequence start
                bytes_after = len(seq_list) - i - 1
                if bytes_after >= 2:
                    return {"complete": True}
                else:
                    return {"complete": False, "first_byte": byte_val, "position": bytes_after + 1}

            elif 0xF0 <= byte_val < 0xF5:
                # 4-byte sequence start
                bytes_after = len(seq_list) - i - 1
                if bytes_after >= 3:
                    return {"complete": True}
                else:
                    return {"complete": False, "first_byte": byte_val, "position": bytes_after + 1}

            elif 0x80 <= byte_val < 0xC0:
                # Continuation byte - keep looking backwards
                continue

            else:
                # Invalid UTF-8 byte (C0, C1, F5-FF)
                # Treat as complete to allow recovery
                if i == len(seq_list) - 1:
                    return {"complete": True}

        # If we've gone through the whole sequence and only found continuation bytes,
        # treat as complete (malformed, but allow recovery)
        return {"complete": True}

    def _valid_continuation_bytes(self, state: dict) -> set[int]:  # noqa: C901
        """
        Get valid continuation bytes based on the current UTF-8 state.

        Args:
            state: UTF-8 state dictionary from _get_utf8_state

        Returns:
            Set of allowed byte values

        Note: Complexity is inherent to UTF-8 specification with multiple byte sequence types.
        """
        first_byte = state["first_byte"]
        position = state["position"]

        # 2-byte sequences: C2-DF
        if 0xC2 <= first_byte < 0xE0:
            if position == 1:
                # Second byte must be 80-BF
                return set(range(0x80, 0xC0))

        # 3-byte sequences
        elif 0xE0 <= first_byte < 0xF0:
            if first_byte == 0xE0:
                # E0: second byte A0-BF, third byte 80-BF
                if position == 1:
                    return set(range(0xA0, 0xC0))
                elif position == 2:
                    return set(range(0x80, 0xC0))

            elif first_byte == 0xED:
                # ED: second byte 80-9F, third byte 80-BF
                if position == 1:
                    return set(range(0x80, 0xA0))
                elif position == 2:
                    return set(range(0x80, 0xC0))

            else:
                # E1-EC, EE-EF: second and third bytes 80-BF
                if position in (1, 2):
                    return set(range(0x80, 0xC0))

        # 4-byte sequences
        elif 0xF0 <= first_byte < 0xF5:
            if first_byte == 0xF0:
                # F0: second byte 90-BF, third and fourth bytes 80-BF
                if position == 1:
                    return set(range(0x90, 0xC0))
                elif position in (2, 3):
                    return set(range(0x80, 0xC0))

            elif first_byte == 0xF4:
                # F4: second byte 80-8F, third and fourth bytes 80-BF
                if position == 1:
                    return set(range(0x80, 0x90))
                elif position in (2, 3):
                    return set(range(0x80, 0xC0))

            else:
                # F1-F3: second, third, and fourth bytes 80-BF
                if position in (1, 2, 3):
                    return set(range(0x80, 0xC0))

        # If we get here, something went wrong - allow all start bytes to recover
        return self._valid_start_bytes()
